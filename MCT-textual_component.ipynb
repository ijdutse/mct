{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCT - textual componet\n",
    "The focus in this notebook is the description of relevant exploratoy analyses for a better understanding of the data. We want to investigate the following in relation to the textual component in *MCT*:\n",
    "* how do to analyse the texts generated by all nodes in $\\mathcal{S}_r$ as a corpus and compare with all other nodes?\n",
    "* the most effective and intuitive way to present/visualise the outputs ...\n",
    "\n",
    "**Textually-Related Clusters** In LDA, documents or text corpus exhibit multiple topics (interlinked or interrealted, not all the time). A node or user posts many tweets covering wide areas or social aspects that matches closely with what other users are dicsussing. For instance, a *hashtag* can be described as a large corpus consisting of many related and unrelated tweets generated using the hashtag as the *anchor*. The interest here is on *how to capture local communties that are formed based on the discussion topics*? We begin by extracting textual contentfrom nodes/users in the structurally-related category. The basic steps include:\n",
    "- For each node in the collection i.e. $v_i \\in \\mathcal{S_r},\\mathcal{S_u}$\n",
    "    - get set of m tweets from each node (as large as possible, making a corpus from the node) such that $$\\mathcal{D}_S = \\{v_i:[t_{i1},t_{i2},t_{i3}, ... ,t_{im}], v_j:[t_{j1},t_{j2},t_{j3}, ... ,t_{jm}] \\} $$ where $\\mathcal{D}_S$ is the data whose nodes have been structurally analysed.\n",
    "- transform text to numeric using the *tfidf-scheme*\n",
    "- regulise (based on L2 and ) the vectors of each transformed document in the corpus to minimise the coefficients in the corpus\n",
    "- apply LDA to analyse topics in the corpus\n",
    "- return the most popular topics in each node's and compare with others in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import json, time, re, os\n",
    "from mlxtend.plotting import ecdf\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler, API, Cursor\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from collections import Counter\n",
    "import sys, json, time, string, re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import preprocessor as p\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim import models, corpora, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authentication keys:\n",
    "access_token = (\"USE YOUR ACCESS_TOKEN FROM TWITTER\")\n",
    "access_token_secret = (\"USE YOUR ACCESS_TOKEN_SECRET FROM TWITTER\")\n",
    "consumer_key = (\"USE YOUR CONSUMER_KEY FROM TWITTER\")\n",
    "consumer_secret = (\"USE YOUR CONSUMER SECRET FROM TWITTER\")\n",
    "# authentication instance:\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) # required for validation\n",
    "auth.set_access_token(access_token, access_token_secret) # required for access after validation of user\n",
    "api =  tweepy.API(auth) # tweepy requires authenticated user to operate\n",
    "# with optional parameters ... to avoid service interruption:\n",
    "#api = tweepy.API(auth_handler=auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29477, 29477, 1995)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# begin with structurally related users and collect finite set of tweets:\n",
    "sr = pd.read_csv('data/mct_structurally_similar_nodes.csv')\n",
    "#nodes to extract texts from:\n",
    "all_nodes = sr.Va_ID  \n",
    "unique_nodes = set(all_nodes) #unique usernames\n",
    "# use small number of queries not to exceed threshold ... \n",
    "users = [i for i in unique_nodes]\n",
    "len(sr), len(all_nodes),len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #use Twitter to extract m-tweets from each node for further analysis ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "visited = []\n",
    "for user in users:\n",
    "    if user in visited:\n",
    "        continue\n",
    "    visited.append(user)\n",
    "    try:\n",
    "        # extract a finite tweets from each node:\n",
    "        extracts1 = defaultdict(list)\n",
    "        for item in tweepy.Cursor(api.user_timeline, id=user).items(100):\n",
    "            extracts1[user].append(item.text)\n",
    "        # convert the extracts to conform to pandas dataframew:\n",
    "        extracts2 = {'UserID':[], 'Tweets':[]}\n",
    "        for k in extracts1.keys():\n",
    "            extracts2['UserID'].append(k)\n",
    "            extracts2['Tweets'].append(extracts1[k])\n",
    "            \n",
    "        #save extracts:\n",
    "        dstr = pd.DataFrame(extracts2) \n",
    "        with open('data/mct_text_extracts_from_sr_nodes.csv','a') as csv:\n",
    "            dstr.to_csv(csv, header=False, mode='a', index_label=False)\n",
    "    \n",
    "    #mitigate service interruption ....              \n",
    "    except tweepy.TweepError:\n",
    "        time.sleep(30 * 15)\n",
    "        continue\n",
    "    except StopIteration:\n",
    "        break\n",
    "stop = time.time()-start\n",
    "print('The process took: ',stop/60, ' minutes')\n",
    "print('no. of actual users: ', len(users))\n",
    "print('no. of visited users: ', len(visited))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #LDA dataset and prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1093985024171540480</td>\n",
       "      <td>['18 people followed me and 5 people unfollowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822379141773148161</td>\n",
       "      <td>['RT @alonso_dm: Nunca habría imaginado que er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1048575348907098113</td>\n",
       "      <td>['@foxandfriends So Geraldo knows more about t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                UserID                                             Tweets\n",
       "0  1093985024171540480  ['18 people followed me and 5 people unfollowe...\n",
       "1   822379141773148161  ['RT @alonso_dm: Nunca habría imaginado que er...\n",
       "2  1048575348907098113  ['@foxandfriends So Geraldo knows more about t..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/mct_text_extracts_from_sr_nodes.csv')\n",
    "df.head(3) # see some more samples: df.Tweets.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing: using custom prepocessing and tweet-preprocessor package\n",
    "punctuation = list(string.punctuation)\n",
    "# define a custom stopset:\n",
    "tweet_stopset = ['rt''rt','\\\\','via',',',':','.','{}','()','[]','\"','\"[','-','=','...',']\"','[\\'\\\"',',', ']','//','/',\\\n",
    "                 'les','des','de','à','…','la','pour','RT',';','....','……','\\'rt\\'','&amp;','&amp']\n",
    "stopset = stopwords.words('english') + punctuation\n",
    "# additinal function to filter irrelevent content and none content words...\n",
    "def mopper(x):\n",
    "    return ' '.join([t.lower() for t in p.clean(x).split() if t not in stopset])#)for t in x.split() if t not in stopset]\n",
    "# a function to stem tokens in the cleanned text using the Proter's Stemmer:\n",
    "stemmer = PorterStemmer()\n",
    "def token_stemmer(text):\n",
    "    # return the stemmed version of tokens that are greater than 2 in length:\n",
    "    return ' '.join([stemmer.stem(token) for token in text.split() if len(token)>2])\n",
    "    #return re.sub(\"[^a-zA-Z]\", \"\", ' '.join([stemmer.stem(token) for token in text.split() if len(token)>2]))\n",
    "\n",
    "#retain alphabets only ...\n",
    "def get_alphabets(text):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a unique and concise ID to each node ... using a short name for ease of operation:\n",
    "df['ID']= ['V'+str(i) for i in range(len(df))]\n",
    "# apply the mopper function to clean the tweets for further analysis ... \n",
    "df['CleanTweets'] = df.Tweets.apply(lambda x: mopper(x))\n",
    "# stem tokens using the token_stemmer:\n",
    "df['CleanTweets'] = df.CleanTweets.apply(lambda x: token_stemmer(x))\n",
    "#retain alphabets only ... \n",
    "df['CleanTweets'] = df.CleanTweets.apply(lambda x: get_alphabets(x))\n",
    "# tokenisation of the clean tweets to be used by the LDA model:\n",
    "df['Shingles'] = df.CleanTweets.apply(lambda x: x.split())\n",
    "df['TweetsLen'] = df.Tweets.apply(lambda x: len([t for t in x.split()]))\n",
    "df['CleanTweetsLen'] = df.CleanTweets.apply(lambda x: len([t for t in x.split()]))\n",
    "#save: \n",
    "df.to_csv('data/mct_K_tweets_extracts_from_sr_nodes.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 813, 6101)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load preprocessed file:\n",
    "df = pd.read_csv('data/mct_K_tweets_extracts_from_sr_nodes.csv')\n",
    "# drop documents lessthan a given threshold, i.e if the tokens are less: ... reduced df (rdf)\n",
    "rdf = df[df.CleanTweetsLen>=700]\n",
    "rdf = rdf[['ID', 'CleanTweets']]\n",
    "len(rdf), len(df), len(df.CleanTweets.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CleanTweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1</td>\n",
       "      <td>de lo nuestros   otro a guerracivilista suelt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V3</td>\n",
       "      <td>all this  incred accurate   this  accurate  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V4</td>\n",
       "      <td>lock her up      rt too mani politician today ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                        CleanTweets\n",
       "0  V1   de lo nuestros   otro a guerracivilista suelt...\n",
       "0  V3    all this  incred accurate   this  accurate  ...\n",
       "0  V4  lock her up      rt too mani politician today ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Affinity Matrix:** to store information about relationship between between pairs based on the degree of topical similarity/distribution ... $\\mathcal{T}_r \\in \\mathcal{M}_{tc_{va}}^{m\\times m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 813)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construction of affinity matrix using the user IDs as columns and index and the entries based on the mean similarity\n",
    "# between pairs ... first off, empty date frame ... with columns and index based on the union of the users ...\n",
    "columns = set(df.ID)\n",
    "index = set(df.ID)\n",
    "tr_afm =pd.DataFrame(np.zeros(shape=(len(index),len(columns))),columns=columns, index=index)\n",
    "# update to the matrix is given later ... \n",
    "tr_afm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V478</th>\n",
       "      <th>V617</th>\n",
       "      <th>V250</th>\n",
       "      <th>V692</th>\n",
       "      <th>V227</th>\n",
       "      <th>V682</th>\n",
       "      <th>V10</th>\n",
       "      <th>V535</th>\n",
       "      <th>V650</th>\n",
       "      <th>V438</th>\n",
       "      <th>...</th>\n",
       "      <th>V280</th>\n",
       "      <th>V756</th>\n",
       "      <th>V197</th>\n",
       "      <th>V26</th>\n",
       "      <th>V397</th>\n",
       "      <th>V550</th>\n",
       "      <th>V180</th>\n",
       "      <th>V351</th>\n",
       "      <th>V781</th>\n",
       "      <th>V230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V478</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V617</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 813 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      V478  V617  V250  V692  V227  V682  V10  V535  V650  V438  ...  V280  \\\n",
       "V478   0.0   0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...   0.0   \n",
       "V617   0.0   0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...   0.0   \n",
       "V250   0.0   0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "      V756  V197  V26  V397  V550  V180  V351  V781  V230  \n",
       "V478   0.0   0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "V617   0.0   0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "V250   0.0   0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 813 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty affinity matrix:\n",
    "tr_afm.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     # Term frequency: topic models such as the LDA is based on the bag of word model (bow)...\n",
    "    # masking training and testing sets: the mask provides a neat way of splitting the data in the correct proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 570, 416, 154)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify the split proportion ... masked df/rdf for data split -- training and testing sets\n",
    "mask = np.random.rand(len(rdf)) < 0.75 # boolean to mask and return dataframe where the value is True\n",
    "# train set:\n",
    "train_rdf = rdf[mask]\n",
    "train_rdf.reset_index(drop=True, inplace=True) # reset the index due to offset from masking ...\n",
    "# test set:\n",
    "test_rdf = rdf[~mask]\n",
    "test_rdf.reset_index(drop=True, inplace=True) # reset index ... \n",
    "len(df),len(rdf),len(train_rdf),len(test_rdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #latent themes/topics in all tweets ...\n",
    "*Using LDA:* Because of high generaton of texts, it will be difficult to view/persepective of the topic, there is a need to represent set of tweets from each node/user and analyse the topics in depth; using a single tweet may not be encompassing, hence the use of aggregate tweets and analyse using LDA. \n",
    "The core function of the *LDA* is finding latent variables in the form of *words distributions* and *topics distributions* in a corpus. In this study, the whole tweet corpus is trained such that each document (set of m tweets from each node) will have a *finite distribution over all topics* (and all *topics will have distribution over words*). It is the distribution of each document we are interested in comparing to find the most similar document; hence, only the highest topics in each docment is used for the comparison. \n",
    "In LDA, a multidimensional space (as a fuction of the number of topics in the corpora) is used to compare document, such that each document is certain distance away/close to the topics. Each topic/document of the node/user is associated with topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of the cleaned and tokenised text:\n",
    "dictionary = corpora.Dictionary([t.split() for t in rdf.CleanTweets]) # a dictionary of terms according to the corpus\n",
    "# generate a corpus based on the dictinary:\n",
    "corpus = [dictionary.doc2bow(doc.split()) for doc in rdf.CleanTweets] # list of tuples denoting doc in the data as BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # the bag-of-word model store a numeric representation of a document in which each term is associated with its frequency of occurence; this is normally given as tuple: (3, 7) where '3' is the token id and '7' is the count or frequency of the token/term in the document/corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7,\n",
       "  '0.087*\"rt\" + 0.046*\"n\" + 0.013*\"the\" + 0.011*\"t\" + 0.009*\"trump\" + 0.008*\"you\" + 0.006*\"amp\" + 0.006*\"it\" + 0.005*\"thi\" + 0.005*\"like\" + 0.005*\"i\" + 0.005*\"say\" + 0.004*\"get\" + 0.004*\"they\" + 0.004*\"peopl\"'),\n",
       " (12,\n",
       "  '0.828*\"nhttps\" + 0.076*\"site\" + 0.050*\"plu\" + 0.014*\"rob\" + 0.005*\"nhelp\" + 0.004*\"n\" + 0.003*\"at\" + 0.003*\"get\" + 0.002*\"cbd\" + 0.000*\"dan\" + 0.000*\"peopl\" + 0.000*\"rt\" + 0.000*\"solution\" + 0.000*\"well\" + 0.000*\"le\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the LDA model:\n",
    "lda = LdaModel(corpus=corpus, num_topics=15, id2word=dictionary, alpha=1e-3,eta=0.5e-3,chunksize=20,\\\n",
    "               minimum_probability=0.0, passes=3)\n",
    "# see topics learnt by the model and top words in each topic:\n",
    "lda.show_topics(num_topics=2,num_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elev', 0.21459636),\n",
       " ('clover', 1.6630345e-05),\n",
       " ('n', 1.6629592e-05),\n",
       " ('the', 1.6627491e-05),\n",
       " ('street', 1.6625303e-05),\n",
       " ('home', 1.662489e-05),\n",
       " ('come', 1.6623913e-05),\n",
       " ('rt', 1.6623711e-05),\n",
       " ('open', 1.6623662e-05),\n",
       " ('new', 1.6623637e-05),\n",
       " ('design', 1.662349e-05),\n",
       " ('famili', 1.6623484e-05),\n",
       " ('welcom', 1.6623477e-05),\n",
       " ('day', 1.6623435e-05),\n",
       " ('south', 1.6623411e-05)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing individual topics for inspection, e.g. inspecting topic 1:\n",
    "# we can make sense of what topic is based on the top words in the topic ...\n",
    "lda.show_topic(topicid=1, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC_1:=> [('elev', 0.21459636), ('clover', 1.6630345e-05), ('n', 1.6629592e-05), ('the', 1.6627491e-05), ('street', 1.6625303e-05), ('home', 1.662489e-05), ('come', 1.6623913e-05), ('rt', 1.6623711e-05), ('open', 1.6623662e-05), ('new', 1.6623637e-05), ('design', 1.662349e-05), ('famili', 1.6623484e-05), ('welcom', 1.6623477e-05), ('day', 1.6623435e-05), ('south', 1.6623411e-05)] \n",
      "\n",
      "TOPIC_2:=> [('via', 0.6472424), ('ad', 0.07462195), ('benefit', 0.07344217), ('well', 0.022859178), ('nat', 0.022079516), ('solut', 0.021466643), ('post', 0.019441694), ('peopl', 0.017609494), ('oil', 0.016286368), ('switch', 0.015354098), ('get', 0.013290652), ('natur', 0.012333833), ('nour', 0.008493414), ('cannabi', 0.0072980933), ('being', 0.004316256)] \n",
      "\n",
      "TOPIC_3:=> [('sugar', 0.27472663), ('holli', 0.027853068), ('uni', 1.4799847e-05), ('easi', 1.4790212e-05), ('rt', 1.4777912e-05), ('n', 1.4776108e-05), ('new', 1.4772304e-05), ('now', 1.4769776e-05), ('the', 1.4768427e-05), ('task', 1.4768094e-05), ('get', 1.4766792e-05), ('player', 1.4766547e-05), ('max', 1.476631e-05), ('great', 1.4766197e-05), ('sale', 1.4765954e-05)] \n",
      "\n",
      "TOPIC_4:=> [('n', 0.43248865), ('rt', 0.08064777), ('m', 0.053837277), ('d', 0.035555143), ('l', 0.03412324), ('u', 0.024834676), ('a', 0.021462915), ('e', 0.019918013), ('r', 0.017833663), ('s', 0.016032688), ('de', 0.015332791), ('le', 0.014759488), ('que', 0.013943888), ('v', 0.012157179), ('p', 0.010264091)] \n",
      "\n",
      "TOPIC_5:=> [('xa', 0.6717711), ('kong', 0.14213207), ('hong', 0.1317245), ('rt', 1.1828927e-06), ('n', 1.1543988e-06), ('nhttps', 1.1513266e-06), ('d', 1.1511756e-06), ('s', 1.1511534e-06), ('t', 1.1511339e-06), ('the', 1.1511257e-06), ('you', 1.1510658e-06), ('brexit', 1.1510515e-06), ('pakistan', 1.1510504e-06), ('propos', 1.151049e-06), ('r', 1.151008e-06)] \n",
      "\n",
      "TOPIC_6:=> [('rt', 0.03314132), ('the', 0.014383931), ('new', 0.010419203), ('today', 0.0090065785), ('thank', 0.008315026), ('amp', 0.008280948), ('we', 0.007649273), ('great', 0.0071470365), ('day', 0.007088394), ('it', 0.0070123305), ('check', 0.0059513235), ('see', 0.005866266), ('one', 0.005781233), ('work', 0.0056004124), ('you', 0.00522244)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspecting n topis according to topic id, say 7 topics:\n",
    "for i in range(1,7):\n",
    "    print('TOPIC_'+str(i)+':=>',lda.show_topic(topicid=i, topn=15), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Topics visualisation...*\n",
    "\n",
    "    # some documents from the corpus for visualisation ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose a random document from the rdf collection\n",
    "random_doc_index = np.random.randint(len(test_rdf)) # random doc from test docs\n",
    "#random_test_doc_index = np.random.randint(len(test_rdf)) # random doc from test docs\n",
    "# express the document as a bag of word model:\n",
    "bow = dictionary.doc2bow(train_rdf.iloc[random_doc_index,1].split()) # 1 denotes the text's column in the df:\n",
    "#topic contributions from the chosen document with random_doc_index\n",
    "doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n",
    "random_doc_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise the output using barplot:\n",
    "fig, ax = plt.subplots(figsize=(9,3));\n",
    "patches = ax.bar(np.arange(len(doc_distribution)),doc_distribution)\n",
    "ax.set_xlabel('Topic ID', fontsize=13)\n",
    "ax.set_ylabel('Topic Contribution', fontsize=13)\n",
    "#ax.set_title('Topic Distibution for document ' + str(random_doc_index), fontsize=17)\n",
    "# use main tokens in the topic as the title:\n",
    "ax.set_title('MAIN TOKENS: '+' '.join([t[0] for t in lda.show_topic(topicid=random_doc_index, topn=13) if len(t[0])>1]))\n",
    "ax.set_xticks(np.linspace(3,30,10))\n",
    "fig.tight_layout()\n",
    "plt.savefig('results/topic-distribution-for-random-train_document.png',dp1=321)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows that *document 70* contributes five different topics in whcih *topic 23* and *topic 24* (whose meaning are given in the next figure) are more relevant. Each document contributes similar patter of topics and the next **figure** shows more results.\n",
    "\n",
    "**Topics' contributions** and corresponding words/terms. The output is interpreted to discern/explain the possible theme/subject based on the terms. For instance, document 24 is possibly about the *US Election Interference Investigation* noting the high weigh given to terms/words such as *Trump* and *Muller*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [('rt', 0.03314132), ('the', 0.014383931), ('new', 0.010419203), ('today', 0.0090065785), ('thank', 0.008315026), ('amp', 0.008280948), ('we', 0.007649273)] \n",
      "\n",
      "7 [('rt', 0.08699513), ('n', 0.045653865), ('the', 0.013029786), ('t', 0.010743319), ('trump', 0.00927815), ('you', 0.0075443625), ('amp', 0.005867165)] \n",
      "\n",
      "4 [('n', 0.43248865), ('rt', 0.08064777), ('m', 0.053837277), ('d', 0.035555143), ('l', 0.03412324), ('u', 0.024834676), ('a', 0.021462915)] \n",
      "\n",
      "5 [('xa', 0.6717711), ('kong', 0.14213207), ('hong', 0.1317245), ('rt', 1.1828927e-06), ('n', 1.1543988e-06), ('nhttps', 1.1513266e-06), ('d', 1.1511756e-06)] \n",
      "\n",
      "14 [('refuge', 0.6648476), ('syrian', 0.22584903), ('priest', 0.022259628), ('cruelti', 0.012287692), ('the', 1.6513967e-06), ('listen', 1.5875232e-06), ('now', 1.5841192e-06)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in doc_distribution.argsort()[-5:][::-1]:\n",
    "    print(i, lda.show_topic(topicid=i, topn=7), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the *trained LDA model* is evaluated on previously unseen data by comparing the distributions of topics from the set of test data. The evaluation is valid only for terms that appear in the train test (represented in the dictionary created earlier) i.e. previously encountered during the training phase. A term is ignored if it's not encountered earlier. Training the model on a larger and diverse set of data will help in making the model more encompassing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get random article from the test set, test_rdf:\n",
    "random_test_doc_index = np.random.randint(len(test_rdf))\n",
    "test_doc_bow = dictionary.doc2bow(test_rdf.iloc[random_test_doc_index,1].split()) # numeric transformation of test set:\n",
    "#test_rdf.iloc[random_test_doc_index,1]\n",
    "random_test_doc_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions from the random document in the test set: \n",
    "test_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=test_doc_bow)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visuliase distribution from the test document using simialr approach:\n",
    "fig, ax = plt.subplots(figsize=(9,3));\n",
    "patches = ax.bar(np.arange(len(test_doc_distribution)), test_doc_distribution)\n",
    "ax.set_xlabel('Topic ID', fontsize=13)\n",
    "ax.set_ylabel('Topic Contribution', fontsize=13)\n",
    "#ax.set_title('Topic Distibution for Test Document ' + str(random_doc_index), fontsize=17)\n",
    "ax.set_title('MAIN TOKENS: '+' '.join([t[0] for t in lda.show_topic(topicid=random_doc_index, topn=13) if len(t[0])>1]))\n",
    "\n",
    "ax.set_xticks(np.linspace(2,16,8))\n",
    "fig.tight_layout()\n",
    "plt.savefig('results/topic-distribution-for-random-test-document.png',dp1=321)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [('rt', 0.03314132), ('the', 0.014383931), ('new', 0.010419203), ('today', 0.0090065785), ('thank', 0.008315026), ('amp', 0.008280948), ('we', 0.007649273)] \n",
      "\n",
      "7 [('rt', 0.08699513), ('n', 0.045653865), ('the', 0.013029786), ('t', 0.010743319), ('trump', 0.00927815), ('you', 0.0075443625), ('amp', 0.005867165)] \n",
      "\n",
      "4 [('n', 0.43248865), ('rt', 0.08064777), ('m', 0.053837277), ('d', 0.035555143), ('l', 0.03412324), ('u', 0.024834676), ('a', 0.021462915)] \n",
      "\n",
      "14 [('refuge', 0.6648476), ('syrian', 0.22584903), ('priest', 0.022259628), ('cruelti', 0.012287692), ('the', 1.6513967e-06), ('listen', 1.5875232e-06), ('now', 1.5841192e-06)] \n",
      "\n",
      "13 [('rt', 2.1176593e-05), ('n', 2.1169322e-05), ('the', 2.1165857e-05), ('it', 2.1165062e-05), ('t', 2.1165055e-05), ('thi', 2.1164939e-05), ('you', 2.1164922e-05)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top terms in the distribution:\n",
    "for i in test_doc_distribution.argsort()[-5:][::-1]:\n",
    "    print(i, lda.show_topic(topicid=i, topn=7), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  dear god  pleas make guidanc clear plain me  might know next step forward  n    concentr god minutes  improv hour follow it  take small god break day   do part uplift others  your smile make immeasur differ someon hurting  allow an   we need pray  need medit allow thought spirit enter minds  must put god first  let neg thought go  watch thought think posit attitude  god forgiv us  practic for   be thank understand god seek insight knowledg regard divin sourc al   god gave everyon guardian angel  the love ask help  ask   n   former wish mani more       repeatedly  confidence  say  feel presenc guardian angel  you least one angel  let knowledg god  love peel away sad uncov innat    n    keep ask guidanc come you  step toward whole alreadi taken  pray     may storm eerienc around planet  help understand storm person lives   n    even pain  look beyond wall find joy sweet knowledg know god love you   when effort successful  give thank it  success meant all us  are separ role impor   prais god joyou word song  allow feel lift heights  you refil  speak act guidanc spirit  godli guidanc harm anyone  good all  pray     all concern give god  god receiv burden improv situat ask help  n    life  pain eerienc posit allow increas god    place concern lovingli hand god  n   remind frequent god out there  also within  everyon els earth  god within all   if can  someth person group   have beautiful  week   when is  help restor world  n    if want peac life  say  peace  want joy  say  joy  you decid many   a decid accustom includ god daili life  pull mind back sacr one when   think posit love thoughts  send divin love around world  from attitude  receiv  n    dear god  pleas make guidanc clear plain me  might know next step forward  n    god provid help challenges  jesu hold hand encourag us  jesu lead ange   let neg thought go  watch thought think posit attitude  god forgiv us  practic for   concentr god minutes  improv hour follow it  take small god break day   keep ask guidanc come you  step toward whole alreadi taken  pray     do part uplift others  your smile make immeasur differ someon hurting  allow an   may storm eerienc around planet  help understand storm person lives   n    we need pray  need medit allow thought spirit enter minds  must put god first  be thank understand god seek insight knowledg regard divin sourc al   let knowledg god  love peel away sad uncov innat    n    god gave everyon guardian angel  the love ask help  ask   n    all concern give god  god receiv burden improv situat ask help  n    repeatedly  confidence  say  feel presenc guardian angel  you least one angel  when effort successful  give thank it  success meant all us  are separ role impor   speak act guidanc spirit  godli guidanc harm anyone  good all  pray     prais god joyou word song  allow feel lift heights  you refil  life  pain eerienc posit allow increas god     have day  look life pleasure  god within you  power enliven you  place concern lovingli hand god  n    even pain  look beyond wall find joy sweet knowledg know god love you   when is  help restor world  n    think posit love thoughts  send divin love around world  from attitude  receiv  n    if want peac life  say  peace  want joy  say  joy  you decid many   dear god  pleas make guidanc clear plain me  might know next step forward  n    a decid accustom includ god daili life  pull mind back sacr one when   so true  god provid help challenges  jesu hold hand encourag us  jesu lead ange   do part uplift others  your smile make immeasur differ someon hurting  allow an  remind frequent god out there  also within  everyon els earth  god within all   may next hour wonder one you  dear one all     know  concentr god minutes  improv hour follow it  take small god break day   keep ask guidanc come you  step toward whole alreadi taken  pray     let neg thought go  watch thought think posit attitude  god forgiv us  practic for   may storm eerienc around planet  help understand storm person lives   n    god gave everyon guardian angel  the love ask help  ask   n    life  pain eerienc posit allow increas god     let knowledg god  love peel away sad uncov innat    n    be thank understand god seek insight knowledg regard divin sourc al  place concern lovingli hand god  n    if roof head  give thank prayer regard loss injuri peopl recent      all concern give god  god receiv burden improv situat ask help  n    rt tweet other you would like tweet  we need pray  need medit allow thought spirit enter minds  must put god first  when effort successful  give thank it  success meant all us  are separ role impor   prais god joyou word song  allow feel lift heights  you refil  when is  help restor world  n    speak act guidanc spirit  godli guidanc harm anyone  good all  pray     think posit love thoughts  send divin love around world  from attitude  receiv  n    even pain  look beyond wall find joy sweet knowledg know god love you   dear god  pleas make guidanc clear plain me  might know next step forward  n    repeatedly  confidence  say  feel presenc guardian angel  you least one angel  a decid accustom includ god daili life  pull mind back sacr one when   let neg thought go  watch thought think posit attitude  god forgiv us  practic for  remind frequent god out there  also within  everyon els earth  god within all   god provid help challenges  jesu hold hand encourag us  jesu lead ange   do part uplift others  your smile make immeasur differ someon hurting  allow an  place concern lovingli hand god  n    if want peac life  say  peace  want joy  say  joy  you decid many   let knowledg god  love peel away sad uncov innat    n    keep ask guidanc come you  step toward whole alreadi taken  pray     god gave everyon guardian angel  the love ask help  ask   n    life  pain eerienc posit allow increas god     may storm eerienc around planet  help understand storm person lives   n    a many  beauti mani countri affected   may angel well surround today  feel there  besid you   concentr god minutes  improv hour follow it  take small god break day   all concern give god  god receiv burden improv situat ask help  n    prais god joyou word song  allow feel lift heights  you refil  be thank understand god seek insight knowledg regard divin sourc al   when effort successful  give thank it  success meant all us  are separ role impor   we need pray  need medit allow thought spirit enter minds  must put god first  think posit love thoughts  send divin love around world  from attitude  receiv  n    when is  help restor world  n  '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rdf.iloc[random_test_doc_index,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does users with structurally similarity discuss different topics, what is their topical orientation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing document**\n",
    "\n",
    "To find similarity between the topics/comparing between using each document is given a meaningful fingerprint for comparison. Because each of the document (or the set of tweets from each node) can be represented as a distribution of contributing topics, the distributions of document is compared for assessing the simiarity between documents in the corpus. Each documnet is compared with other documents in the corpuse and the most similar documents to the document (known as the *anchor document*) are returned. \n",
    "Given that the comparison is made on probability distribution $p(\\cdot)$, the *Jensen-Shannon Divergence* is used to measure the distance between topical themes. The *Jensen-Shannon* is a suitable statistical metric to measure the distance/simialriy between the documents using their distributions. \n",
    "For each corpus/document of the user, the model produce the topics and make comparison with other users the JSD is used. \n",
    "The divergence in the distirbutions of the document is used to assess simialrity. Proximity is crucial in this case - the closer or similar the document, the less the divergence/distance and vice versa.\n",
    "\n",
    "***Jensen-Shannon Metric*** is a symmetrical version of the asymmetric *Kullback-Leibler Divergence*. The symmerical property is relevant since the task of comparing two documents should be the same irrespective of the order - A-->B or B-->A should be the same; the *Jensen-Shannon* meets this requirement. Given two discrete distributions $A$ and $B$, the *Jensen-Shannon Divergence (JSD)* is given by/define by $$JS_{divergence}(A||B) = \\frac{1}{2}D(A||M) + \\frac{1}{2}D(B||M)$$\n",
    "where $$M = \\frac{1}{2}(A+B)$$ denotes the mean of the distributons and the *Kullback-Leibler Divergence D* given by\n",
    "$$D(A||B) = \\sum_i A(i)\\log\\frac{A(i)}{B(i)}$$\n",
    "and substituted in **Eq. 1**\n",
    "\n",
    "$$JS_{divergence}(A||B) = \\frac{1}{2}\\sum_i\\left[A(i)\\log\\left(\\frac{A(i)}{\\frac{1}{2}(A(i)+B(i))}\\right) + B(i)\\log\\left(\\frac{B(i)}{\\frac{1}{2}(A(i)+B(i))}\\right)\\right]$$\n",
    "\n",
    "The distance measure of *JSD* is obtained by squaring its divergence relation given by $$JS_{dist} = \\sqrt{ \\left(JS_{divergence}(A||B \\right)}$$\n",
    "\n",
    "Both $(JS_{divergence}(A||B)$ and $JS_{dist}$ have been computed using the *Scipy implementation* (see https://www.scipy.org/) which is based on the entropy (which calculates the K-L divergence $(JS_{divergence}(A||B)$.)\n",
    "\n",
    "***Matrix of Values*** In order ro enable faster/efficient computation noting the huge size of the data), all the topics distributions learned by the topic model are represented in a dense matrix $M_{lda}^{m\\times k}$. The matrix $M_{lda}^{m\\times k}$ of size $M\\times K$ is a dense matrix consisting of $M$ number of documents (set of tweets from nodes) and $K$ their corresponding number of *topics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 15)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dense matrix creation:\n",
    "doc_topic_dist = np.array([[tup[1] for tup in t1] for t1 in lda[corpus]])\n",
    "doc_topic_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most similar documents to the achor document constitute the circles or set of textually similar items with the anchor,\n",
    "#hence belonging to same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the matrix, e.g. is 502, 15 i.e. 207 documents and 25 topics. In other words, all the discussions topics in the 207 nodes is within the set of 25 topics and each node may discus from 0 to 25 topics, hence similar nodes will discuss simialr topics. The goal is to identify clsuters with high degree of similarity.\n",
    "\n",
    "*Helper functions:*\n",
    "- A function to compute the *JS distance*. The function accepts a *query document* and compare with all other *documents* in the matrix. Because the matrix is an $M \\times K$ matrix, the output from issuing the query will return an array of size $M$ i.e. size of the corpus or number of documents in the corpus each with a degree of similarity with the query document. Only the top similar documenst are used in this study.\n",
    "- A function to return the *indices of top similar documents* in the matrix to the anchor document. The indices can be used to compare/see the returned document by checking the train_rdf for the specific documents. Using the distribution of the random document and all other distributions in the dense matrix using the indices of the most similar docs to the anchor doc are returned.\n",
    "\n",
    "*Final update to the affinity matrix...*\n",
    "\n",
    "    # the affinity matrix created earlier is updated according to the degree of similarity in the topical distributions of the respective nodes. The data in the matrix is then used for clustering ... \n",
    "    # because the comparison space is huge, ~100k for each comparison, the process is paralellise for efficiency ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v1 in tr_afm.index:\n",
    "    for v2 in tr_afm.columns:\n",
    "        # convert to string, tokenise and return list of raw tokens:\n",
    "        c1=df[df.ID.values==v1].CleanTweets.str.split().tolist()[0]\n",
    "        c2=df[df.ID.values==v2].CleanTweets.str.split().tolist()[0]\n",
    "        # documents bag of words ... bow for individual document:\n",
    "        db1 = dictionary.doc2bow(c1)\n",
    "        db2 = dictionary.doc2bow(c2)\n",
    "        # topics distributions in the documents (based on the bow):\n",
    "        tdist1 = np.array([tup[1] for tup in lda.get_document_topics(bow=db1)])\n",
    "        tdist2 = np.array([tup[1] for tup in lda.get_document_topics(bow=db2)])\n",
    "        # compute similarity:\n",
    "        dsim = np.round(distance.jensenshannon(tdist1, tdist2), 3)\n",
    "        # assign or populate the affinity matrix:\n",
    "        tr_afm.loc[v1,v2]=dsim\n",
    "# save file: tr_afm.to_csv('data/affinity_matrix_of_tr.csv', index_label = False)\n",
    "tr_afm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 813)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_afm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V478</th>\n",
       "      <th>V617</th>\n",
       "      <th>V250</th>\n",
       "      <th>V692</th>\n",
       "      <th>V227</th>\n",
       "      <th>V682</th>\n",
       "      <th>V10</th>\n",
       "      <th>V535</th>\n",
       "      <th>V650</th>\n",
       "      <th>V438</th>\n",
       "      <th>...</th>\n",
       "      <th>V280</th>\n",
       "      <th>V756</th>\n",
       "      <th>V197</th>\n",
       "      <th>V26</th>\n",
       "      <th>V397</th>\n",
       "      <th>V550</th>\n",
       "      <th>V180</th>\n",
       "      <th>V351</th>\n",
       "      <th>V781</th>\n",
       "      <th>V230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V478</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V617</th>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V250</th>\n",
       "      <td>0.208</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V692</th>\n",
       "      <td>0.248</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V227</th>\n",
       "      <td>0.482</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 813 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       V478   V617   V250   V692   V227   V682    V10   V535   V650   V438  \\\n",
       "V478  0.000  0.171  0.208  0.248  0.482  0.159  0.595  0.367  0.164  0.531   \n",
       "V617  0.171    NaN  0.227  0.304  0.430  0.076  0.551  0.438  0.143  0.503   \n",
       "V250  0.208  0.227  0.000  0.351  0.424  0.238  0.537  0.473  0.247  0.490   \n",
       "V692  0.248  0.304  0.351    NaN  0.437  0.323  0.535  0.201  0.352  0.460   \n",
       "V227  0.482  0.430  0.424  0.437  0.000  0.485  0.160  0.588  0.536  0.124   \n",
       "\n",
       "      ...   V280   V756   V197    V26   V397   V550   V180   V351   V781  \\\n",
       "V478  ...  0.338  0.180  0.487  0.374  0.539  0.570  0.392  0.249  0.142   \n",
       "V617  ...  0.286  0.077  0.433  0.380  0.553  0.527  0.337  0.166  0.229   \n",
       "V250  ...  0.311  0.210  0.432  0.390  0.539  0.509  0.333  0.239  0.285   \n",
       "V692  ...  0.323  0.294  0.440  0.243  0.378  0.514  0.372  0.296  0.341   \n",
       "V227  ...  0.170  0.389  0.023  0.233  0.305  0.124  0.114  0.283  0.591   \n",
       "\n",
       "       V230  \n",
       "V478  0.160  \n",
       "V617  0.127  \n",
       "V250  0.218  \n",
       "V692  0.250  \n",
       "V227  0.381  \n",
       "\n",
       "[5 rows x 813 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_afm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MCT clustering:**\n",
    "    # having obtain the affinity matrix, the next stage is to apply the MCT algorithm to identify or detect relevant clusters in the network/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

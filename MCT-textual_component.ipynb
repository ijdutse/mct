{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCT - textual componet\n",
    "The focus in this notebook is the description of relevant exploratoy analyses for a better understanding of the data. We want to investigate the following in relation to the textual component in *MCT*:\n",
    "* how do to analyse the texts generated by all nodes in $\\mathcal{S}_r$ as a corpus and compare with all other nodes?\n",
    "* the most effective and intuitive way to present/visualise the outputs ...\n",
    "\n",
    "**Textually-Related Clusters** In LDA, documents or text corpus exhibit multiple topics (interlinked or interrealted, not all the time). A node or user posts many tweets covering wide areas or social aspects that matches closely with what other users are dicsussing. For instance, a *hashtag* can be described as a large corpus consisting of many related and unrelated tweets generated using the hashtag as the *anchor*. The interest here is on *how to capture local communties that are formed based on the discussion topics*? We begin by extracting textual contentfrom nodes/users in the structurally-related category. The basic steps include:\n",
    "- For each node in the collection i.e. $v_i \\in \\mathcal{S_r},\\mathcal{S_u}$\n",
    "    - get set of m tweets from each node (as large as possible, making a corpus from the node) such that $$\\mathcal{D}_S = \\{v_i:[t_{i1},t_{i2},t_{i3}, ... ,t_{im}], v_j:[t_{j1},t_{j2},t_{j3}, ... ,t_{jm}] \\} $$ where $\\mathcal{D}_S$ is the data whose nodes have been structurally analysed.\n",
    "- transform text to numeric using the *tfidf-scheme*\n",
    "- regulise (based on L2 and ) the vectors of each transformed document in the corpus to minimise the coefficients in the corpus\n",
    "- apply LDA to analyse topics in the corpus\n",
    "- return the most popular topics in each node's and compare with others in the corpus\n",
    "\n",
    "*Why the topic models?* (1) Because of high generaton of texts, it will be difficult to view/persepective of the topic, there is a need to represent set of tweets from each node/user and analyse the topics in depth (2) using a single tweet may not be encompassing, hence the use of aggregate tweets and analyse using LDA. **Figure xxx** shows the LDA process pipeline and **Figure xxx** shows a simplified *plate notation* of the LDA. To find similarity between the topics/comparing between using each document is given a meaningful fingerprint for comparison. In LDA, a multidimensional space (as a fuction of the number of topics in the corpora) is used to compare document. The space in LDA is a *simplex* (see **Figure xxx**) such that each document is certain distance away/close to the topics. Each topic/document of the node/user is associated with topics in the *simplex space* -- the closer a document is to the corner of the topic in the space, the more similar.\n",
    "**Similarity Measure:** Given that the comparison is made on probability distribution $p(\\cdot)$, the *Jensen-Shannon Divergence* is used to measure the distance between topical themes. For each corpus/document of the user, the model produce the topics and make comparison with other users the JSD is used. The shorter the distance, the more similar.\n",
    "\n",
    "**:::** *Does users with structurally similarity discuss different topics, what is their topical orientation?*\n",
    "\n",
    "**LSA - analysing the topics discussed by users**\n",
    "\n",
    "*LDA* is a generative model that assigns word distributions to topics and topic distributions to documents (many overlapps are bound to happen in both the distributions) based on unsupervised learning paradigm. This is to undestand the specific topics being discussed and which of the topics attract much attention. The focus is on the use Latent semantic analysis to summarised the different topics and categorise into relevant themes/groups and the level of interest based on the number of topics and related ones in the themes. We apply LSA to conduct this experiment and Table xx summarise out results. The most attractive topicof discussion is ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import json, time, re, os\n",
    "from mlxtend.plotting import ecdf\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler, API, Cursor\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from collections import Counter\n",
    "import sys, json, time, string, re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import preprocessor as p\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim import models, corpora, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authentication keys:\n",
    "access_token = (\"USE YOUR ACCESS_TOKEN FROM TWITTER\")\n",
    "access_token_secret = (\"USE YOUR ACCESS_TOKEN_SECRET FROM TWITTER\")\n",
    "consumer_key = (\"USE YOUR CONSUMER_KEY FROM TWITTER\")\n",
    "consumer_secret = (\"USE YOUR CONSUMER SECRET FROM TWITTER\")\n",
    "# authentication instance:\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) # required for validation\n",
    "auth.set_access_token(access_token, access_token_secret) # required for access after validation of user\n",
    "api =  tweepy.API(auth) # tweepy requires authenticated user to operate\n",
    "# with optional parameters ... to avoid service interruption:\n",
    "#api = tweepy.API(auth_handler=auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29477, 29477, 1995)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# begin with structurally related users and collect finite set of tweets:\n",
    "sr = pd.read_csv('data/mct_structurally_similar_nodes.csv')\n",
    "#nodes to extract texts from:\n",
    "all_nodes = sr.Va_ID  \n",
    "unique_nodes = set(all_nodes) #unique usernames\n",
    "# use small number of queries not to exceed threshold ... \n",
    "users = [i for i in unique_nodes]\n",
    "len(sr), len(all_nodes),len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #use Twitter to extract m-tweets from each node for further analysis ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "visited = []\n",
    "for user in users:\n",
    "    if user in visited:\n",
    "        continue\n",
    "    visited.append(user)\n",
    "    try:\n",
    "        # extract a finite tweets from each node:\n",
    "        extracts1 = defaultdict(list)\n",
    "        for item in tweepy.Cursor(api.user_timeline, id=user).items(100):\n",
    "            extracts1[user].append(item.text)\n",
    "        # convert the extracts to conform to pandas dataframew:\n",
    "        extracts2 = {'UserID':[], 'Tweets':[]}\n",
    "        for k in extracts1.keys():\n",
    "            extracts2['UserID'].append(k)\n",
    "            extracts2['Tweets'].append(extracts1[k])\n",
    "            \n",
    "        #save extracts:\n",
    "        dstr = pd.DataFrame(extracts2) \n",
    "        with open('data/mct_text_extracts_from_sr_nodes.csv','a') as csv:\n",
    "            dstr.to_csv(csv, header=False, mode='a', index_label=False)\n",
    "    \n",
    "    #mitigate service interruption ....              \n",
    "    except tweepy.TweepError:\n",
    "        time.sleep(30 * 15)\n",
    "        continue\n",
    "    except StopIteration:\n",
    "        break\n",
    "stop = time.time()-start\n",
    "print('The process took: ',stop/60, ' minutes')\n",
    "print('no. of actual users: ', len(users))\n",
    "print('no. of visited users: ', len(visited))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #LDA dataset and prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1093985024171540480</td>\n",
       "      <td>['18 people followed me and 5 people unfollowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822379141773148161</td>\n",
       "      <td>['RT @alonso_dm: Nunca habría imaginado que er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1048575348907098113</td>\n",
       "      <td>['@foxandfriends So Geraldo knows more about t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                UserID                                             Tweets\n",
       "0  1093985024171540480  ['18 people followed me and 5 people unfollowe...\n",
       "1   822379141773148161  ['RT @alonso_dm: Nunca habría imaginado que er...\n",
       "2  1048575348907098113  ['@foxandfriends So Geraldo knows more about t..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/mct_text_extracts_from_sr_nodes.csv')\n",
    "df.head(3) # see some more samples: df.Tweets.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing: using custom prepocessing and tweet-preprocessor package\n",
    "punctuation = list(string.punctuation)\n",
    "# define a custom stopset:\n",
    "tweet_stopset = ['rt''rt','\\\\','via',',',':','.','{}','()','[]','\"','\"[','-','=','...',']\"','[\\'\\\"',',', ']','//','/',\\\n",
    "                 'les','des','de','à','…','la','pour','RT',';','....','……','\\'rt\\'','&amp;','&amp']\n",
    "stopset = stopwords.words('english') + punctuation\n",
    "# additinal function to filter irrelevent content and none content words...\n",
    "def mopper(x):\n",
    "    return ' '.join([t.lower() for t in p.clean(x).split() if t not in stopset])#)for t in x.split() if t not in stopset]\n",
    "# a function to stem tokens in the cleanned text using the Proter's Stemmer:\n",
    "stemmer = PorterStemmer()\n",
    "def token_stemmer(text):\n",
    "    # return the stemmed version of tokens that are greater than 2 in length:\n",
    "    return ' '.join([stemmer.stem(token) for token in text.split() if len(token)>2])\n",
    "    #return re.sub(\"[^a-zA-Z]\", \"\", ' '.join([stemmer.stem(token) for token in text.split() if len(token)>2]))\n",
    "\n",
    "#retain alphabets only ...\n",
    "def get_alphabets(text):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the mopper function to clean the tweets for further analysis ... \n",
    "df['CleanTweets'] = df.Tweets.apply(lambda x: mopper(x))\n",
    "# stem tokens using the token_stemmer:\n",
    "df['CleanTweets'] = df.CleanTweets.apply(lambda x: token_stemmer(x))\n",
    "#retain alphabets only ... \n",
    "df['CleanTweets'] = df.CleanTweets.apply(lambda x: get_alphabets(x))\n",
    "# tokenisation of the clean tweets to be used by the LDA model:\n",
    "df['Shingles'] = df.CleanTweets.apply(lambda x: x.split())\n",
    "df['TweetsLen'] = df.Tweets.apply(lambda x: len([t for t in x.split()]))\n",
    "df['CleanTweetsLen'] = df.CleanTweets.apply(lambda x: len([t for t in x.split()]))\n",
    "#save: \n",
    "df.to_csv('data/mct_M_tweets_from_sr_nodes.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 174, 10937)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load preprocessed file:\n",
    "#df = pd.read_csv('data/mct_100_tweets_from_sr_nodes.csv')\n",
    "df = pd.read_csv('data/mct_M_tweets_from_sr_nodes.csv')\n",
    "# drop documents lessthan a given threshold, i.e if the tokens are less: ... reduced df (rdf)\n",
    "rdf = df[df.CleanTweetsLen>=700]\n",
    "len(rdf), len(df), len(df.Tweets.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>TweetsLen</th>\n",
       "      <th>CleanTweetsLen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>129.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.960535e+17</td>\n",
       "      <td>1593.953488</td>\n",
       "      <td>953.961240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.267478e+17</td>\n",
       "      <td>229.281243</td>\n",
       "      <td>168.563898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.585205e+07</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>702.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.496433e+08</td>\n",
       "      <td>1451.000000</td>\n",
       "      <td>836.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.376033e+09</td>\n",
       "      <td>1589.000000</td>\n",
       "      <td>941.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.656181e+17</td>\n",
       "      <td>1741.000000</td>\n",
       "      <td>1039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.116606e+18</td>\n",
       "      <td>2193.000000</td>\n",
       "      <td>1476.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             UserID    TweetsLen  CleanTweetsLen\n",
       "count  1.290000e+02   129.000000      129.000000\n",
       "mean   2.960535e+17  1593.953488      953.961240\n",
       "std    4.267478e+17   229.281243      168.563898\n",
       "min    1.585205e+07   968.000000      702.000000\n",
       "25%    3.496433e+08  1451.000000      836.000000\n",
       "50%    2.376033e+09  1589.000000      941.000000\n",
       "75%    7.656181e+17  1741.000000     1039.000000\n",
       "max    1.116606e+18  2193.000000     1476.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Term frequency: topic models such as the *LDA* is based on the bag of word model (bow)...\n",
    "    # masking training and testing sets: the mask provides a neat way of splitting the data. In this instance, 75 is available for training and 25 for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 129, 103, 26)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify the split proportion ... masked df/rdf for data split -- training and testing sets\n",
    "mask = np.random.rand(len(rdf)) < 0.75 # boolean to mask and return dataframe where the value is True\n",
    "# train set:\n",
    "train_rdf = rdf[mask]\n",
    "train_rdf.reset_index(drop=True, inplace=True) # reset the index due to offset from masking ...\n",
    "# test set:\n",
    "test_rdf = rdf[~mask]\n",
    "test_rdf.reset_index(drop=True, inplace=True) # reset index ... \n",
    "len(df),len(rdf),len(train_rdf),len(test_rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent themes/topics ... *based on all tweets*; then apply topic model: LSA/LDA ... \n",
    "#To achieve this goal, we aggregate may tweets within a given period to improve the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #latent themes/topics in all tweets ...\n",
    "*Using LDA:* The core function of the *LDA* is finding latent variables in the form of *words distributions* and *topics distributions* in a corpus. In this study, the whole tweet corpus is trained such that each document (set of *m* tweets from each node) will have a *finite distribution over all topics* (and all *topics will have distribution over words*). It is the distribution of each document we are interested in comparing to find the most similar document; hence, only the highest topics in each docment is used for the comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of the cleaned and tokenised text:\n",
    "dictionary = corpora.Dictionary(rdf.Shingles) # a dictionary of terms according to the corpus\n",
    "# generate a corpus based on the dictinary:\n",
    "corpus = [dictionary.doc2bow(doc) for doc in rdf.Shingles] # list of tuples denoting doc in the data as bag-of-words(BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # the bag-of-word model store a numeric representation of a document in which each term is associated with its frequency of occurence; this is normally given as tuple: (3, 7) where '3' is the token id and '7' is the count or frequency of the token/term in the document/corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4,\n",
       "  '0.067*\"grand\" + 0.065*\"injur\" + 0.062*\"mer\" + 0.060*\"cream\" + 0.050*\"saudi\" + 0.050*\"desir\" + 0.044*\"india\" + 0.036*\"n\" + 0.030*\"promis\" + 0.026*\"recover\" + 0.025*\"parliament\" + 0.024*\"nb\" + 0.024*\"pakistan\" + 0.023*\"shame\" + 0.021*\"indian\" + 0.020*\"deleg\" + 0.020*\"tail\" + 0.019*\"dinner\" + 0.018*\"npeopl\" + 0.016*\"arabia\" + 0.015*\"condemn\" + 0.014*\"rt\" + 0.014*\"pm\" + 0.013*\"maa\" + 0.012*\"interior\"'),\n",
       " (12,\n",
       "  '0.081*\"the\" + 0.051*\"syrian\" + 0.039*\"now\" + 0.036*\"film\" + 0.032*\"listen\" + 0.025*\"playing\" + 0.020*\"space\" + 0.017*\"nhttps\" + 0.014*\"new\" + 0.014*\"den\" + 0.012*\"scotland\" + 0.011*\"cultur\" + 0.011*\"juli\" + 0.011*\"with\" + 0.011*\"print\" + 0.010*\"journey\" + 0.010*\"for\" + 0.010*\"smoke\" + 0.009*\"light\" + 0.009*\"bomb\" + 0.009*\"green\" + 0.008*\"blue\" + 0.008*\"rose\" + 0.008*\"feminist\" + 0.008*\"edit\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the LDA model:\n",
    "lda = LdaModel(corpus=corpus, num_topics=25, id2word=dictionary, alpha=1e-3,eta=0.5e-3,chunksize=20,\\\n",
    "               minimum_probability=0.0, passes=3)\n",
    "# see topics learnt by the model and top words in each topic:\n",
    "lda.show_topics(num_topics=2,num_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('traitor', 0.2891998),\n",
       " ('disappoint', 0.2745399),\n",
       " ('dare', 0.1313995),\n",
       " ('cold', 0.090121984),\n",
       " ('calori', 0.028188678),\n",
       " ('hunger', 0.016523646),\n",
       " ('oath', 0.015510962),\n",
       " ('hurl', 0.0061875107),\n",
       " ('rt', 0.0031037321),\n",
       " ('t', 0.00042930167),\n",
       " ('n', 0.00020122278),\n",
       " ('you', 0.00019497899),\n",
       " ('the', 0.00017042362),\n",
       " ('calm', 0.0001671819),\n",
       " ('like', 0.00013433999),\n",
       " ('one', 0.0001231407),\n",
       " ('don', 0.000121288),\n",
       " ('trump', 0.0001175727),\n",
       " ('say', 0.00011084763),\n",
       " ('peopl', 9.433629e-05),\n",
       " ('it', 9.226348e-05)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing individual topics for inspection, e.g. inspecting topic 1:\n",
    "# we can make sense of what topic is based on the top words in the topic ...\n",
    "lda.show_topic(topicid=1, topn=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting n topis according to topic id, say 7 topics:\n",
    "for i in range(1,7):\n",
    "    print('TOPIC_'+str(i)+':=>',lda.show_topic(topicid=i, topn=21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Topics visualisation***\n",
    "\n",
    "    # some documents from the corpus (the training set) for visualisation ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random document from the rdf collection\n",
    "random_doc_index = np.random.randint(len(train_rdf))\n",
    "# express the document as a bag of word model:\n",
    "bow = dictionary.doc2bow(train_rdf.iloc[random_doc_index,3]) # 3 denotes the shingle's column in the dataframe:\n",
    "#choose an arbitrary document based on its index in the dataframe: train_rdf.iloc[random_doc_index,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic contributions from the chosen document with random_doc_index\n",
    "doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # visualise the output using barplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,3));\n",
    "patches = ax.bar(np.arange(len(doc_distribution)),doc_distribution)\n",
    "ax.set_xlabel('Topic ID', fontsize=13)\n",
    "ax.set_ylabel('Topic Contribution', fontsize=13)\n",
    "#ax.set_title('Topic Distibution for document ' + str(random_doc_index), fontsize=17)\n",
    "# use main tokens in the topic as the title:\n",
    "ax.set_title('Main tokens: '+' '.join([t[0] for t in lda.show_topic(topicid=random_doc_index, topn=13) if len(t[0])>1]))\n",
    "\n",
    "\n",
    "ax.set_xticks(np.linspace(3,30,10))\n",
    "fig.tight_layout()\n",
    "#plt.savefig('results/topic-distribution-for-random-document.png',dp1=321)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows that *document 70* contributes five different topics in whcih *topic 23* and *topic 24* (whose meaning are given in the next figure) are more relevant. Each document contributes similar patter of topics and the next **figure** shows more results.\n",
    "\n",
    "**Topics' contributions** and corresponding words/terms. The output is interpreted to discern/explain the possible theme/subject based on the terms. For instance, document 24 is possibly about the *US Election Interference Investigation* noting the high weigh given to terms/words such as *Trump* and *Muller*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [('rt', 0.071895674), ('n', 0.01687156), ('the', 0.01575746), ('t', 0.008352737), ('trump', 0.006973596), ('amp', 0.0069507626), ('it', 0.006302818)] \n",
      "\n",
      "7 [('n', 0.87550956), ('you', 0.019605702), ('racist', 0.017237892), ('na', 0.011738921), ('support', 0.007219665), ('nwhat', 0.006757225), ('that', 0.006657888)] \n",
      "\n",
      "0 [('rt', 0.108258896), ('que', 0.08544535), ('n', 0.08023813), ('a', 0.04262468), ('d', 0.037150234), ('m', 0.03693572), ('est', 0.03190444)] \n",
      "\n",
      "12 [('the', 0.08124576), ('syrian', 0.05086078), ('now', 0.039046958), ('film', 0.035821207), ('listen', 0.03156211), ('playing', 0.025355732), ('space', 0.020406995)] \n",
      "\n",
      "4 [('grand', 0.06654319), ('injur', 0.065450594), ('mer', 0.06155301), ('cream', 0.06028571), ('saudi', 0.050489184), ('desir', 0.049929406), ('india', 0.04376798)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in doc_distribution.argsort()[-5:][::-1]:\n",
    "    print(i, lda.show_topic(topicid=i, topn=7), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the *trained LDA model* is evaluated on previously unseen data by comparing the distributions of topics from the set of test data. The evaluation is valid only for terms that appear in the train test (represented in the dictionary created earlier) i.e. previously encountered during the training phase. A term is ignored if it's not encountered earlier. Training the model on a larger and diverse set of data will help in making the model more encompassing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get random article from the test set, test_rdf:\n",
    "random_test_doc_index = np.random.randint(len(test_rdf))\n",
    "random_test_doc_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'soro',\n",
       " 'funded',\n",
       " 'fascist',\n",
       " 'cia',\n",
       " 'gay',\n",
       " 'nazi',\n",
       " 'russophobic',\n",
       " 'imperialist',\n",
       " 'repeatedli',\n",
       " 'head',\n",
       " 'but',\n",
       " 'innoc',\n",
       " 'riot',\n",
       " 'stick',\n",
       " 'n',\n",
       " 'rt',\n",
       " 'read',\n",
       " 'headlin',\n",
       " 'first',\n",
       " 'mitch',\n",
       " 'mcconnel',\n",
       " 'russian',\n",
       " 'asshat',\n",
       " 'still',\n",
       " 'got',\n",
       " 'point',\n",
       " 'across',\n",
       " 'rt',\n",
       " 'i',\n",
       " 'm',\n",
       " 'racist',\n",
       " 'like',\n",
       " 'play',\n",
       " 'golf',\n",
       " 'restrict',\n",
       " 'countri',\n",
       " 'club',\n",
       " 'gate',\n",
       " 'commun',\n",
       " 'countri',\n",
       " 'wall',\n",
       " 'arou',\n",
       " 'rt',\n",
       " 'new',\n",
       " 'twitter',\n",
       " 'friends',\n",
       " 'thank',\n",
       " 'follow',\n",
       " 'thi',\n",
       " 'place',\n",
       " 'devot',\n",
       " 'illumin',\n",
       " 'augment',\n",
       " 'gross',\n",
       " 'exagger',\n",
       " 'rt',\n",
       " 'the',\n",
       " 'evid',\n",
       " 'got',\n",
       " 'solid',\n",
       " 'impeach',\n",
       " 'presid',\n",
       " 'wouldn',\n",
       " 't',\n",
       " 'tear',\n",
       " 'countri',\n",
       " 'apart',\n",
       " 'said',\n",
       " 'n',\n",
       " 'rt',\n",
       " 'new',\n",
       " 'favorit',\n",
       " 'border',\n",
       " 'crossing',\n",
       " 'oh',\n",
       " 'canada',\n",
       " 'rt',\n",
       " 'what',\n",
       " 'mug',\n",
       " 'rt',\n",
       " 'chuck',\n",
       " 'todd',\n",
       " 'continu',\n",
       " 'presenc',\n",
       " 'proof',\n",
       " 'embalm',\n",
       " 'remain',\n",
       " 'inexact',\n",
       " 'science',\n",
       " 'rt',\n",
       " 'though',\n",
       " 'britain',\n",
       " 'boorish',\n",
       " 'johnson',\n",
       " 'plan',\n",
       " 'sever',\n",
       " 'ireland',\n",
       " 'two',\n",
       " 'yet',\n",
       " 'bother',\n",
       " 'talk',\n",
       " 'anyon',\n",
       " 'dublin',\n",
       " 'rt',\n",
       " 'unfair',\n",
       " 'poss',\n",
       " 'comitatu',\n",
       " 'rule',\n",
       " 'won',\n",
       " 't',\n",
       " 'let',\n",
       " 'trump',\n",
       " 'have',\n",
       " 'polic',\n",
       " 'state',\n",
       " 'hi',\n",
       " 'dream',\n",
       " 'rt',\n",
       " 'jerri',\n",
       " 'nadler',\n",
       " 'whip',\n",
       " 'some',\n",
       " 'delici',\n",
       " 'peach',\n",
       " 'mint',\n",
       " 'thi',\n",
       " 'weekend',\n",
       " 'rt',\n",
       " 'want',\n",
       " 'impeach',\n",
       " 'gotta',\n",
       " 'make',\n",
       " 'call',\n",
       " 'rt',\n",
       " 'economist',\n",
       " 'walk',\n",
       " 'bar',\n",
       " 'there',\n",
       " 'uncertainti',\n",
       " 'punchline',\n",
       " 'rt',\n",
       " 'american',\n",
       " 'place',\n",
       " 'intern',\n",
       " 'sanction',\n",
       " 'shame',\n",
       " 'global',\n",
       " 'way',\n",
       " 'pronounc',\n",
       " 'niche',\n",
       " 'rt',\n",
       " 'most',\n",
       " 'artist',\n",
       " 'tri',\n",
       " 'repres',\n",
       " 'subject',\n",
       " 'faithfully',\n",
       " 'cindi',\n",
       " 'sherman',\n",
       " 'work',\n",
       " 'built',\n",
       " 'lie',\n",
       " 'fr',\n",
       " 'rt',\n",
       " 'it',\n",
       " 'academ',\n",
       " 'write',\n",
       " 'procrastination',\n",
       " 'it',\n",
       " 'preemptiv',\n",
       " 'withhold',\n",
       " 'linguist',\n",
       " 'capit',\n",
       " 'time',\n",
       " 'anxiety',\n",
       " 'rt',\n",
       " 'scienc',\n",
       " 'true',\n",
       " 'whether',\n",
       " 'believ',\n",
       " 'it',\n",
       " 'religion',\n",
       " 'true',\n",
       " 'whether',\n",
       " 'true',\n",
       " 'rt',\n",
       " 'tunisi',\n",
       " 'perd',\n",
       " 'grand',\n",
       " 'pr',\n",
       " 'sident',\n",
       " 'nou',\n",
       " 'perdon',\n",
       " 'ami',\n",
       " 'le',\n",
       " 'combat',\n",
       " 'b',\n",
       " 'ji',\n",
       " 'ca',\n",
       " 'd',\n",
       " 'essebsi',\n",
       " 'sont',\n",
       " 'pa',\n",
       " 'termin',\n",
       " 's',\n",
       " 'c',\n",
       " 'est',\n",
       " 'a',\n",
       " 'rt',\n",
       " 'two',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'today',\n",
       " 'presid',\n",
       " 'trump',\n",
       " 'announc',\n",
       " 'tran',\n",
       " 'militari',\n",
       " 'ban',\n",
       " 'su',\n",
       " 'still',\n",
       " 'court',\n",
       " 'fight',\n",
       " 'clients',\n",
       " 'i',\n",
       " 'rt',\n",
       " 'd',\n",
       " 'c',\n",
       " 'lack',\n",
       " 'statehood',\n",
       " 'mean',\n",
       " 'american',\n",
       " 'live',\n",
       " 'don',\n",
       " 't',\n",
       " 'vote',\n",
       " 'member',\n",
       " 'congress',\n",
       " 'rt',\n",
       " 'mitch',\n",
       " 'mcconnel',\n",
       " 'reportedli',\n",
       " 'receiv',\n",
       " 'donat',\n",
       " 'vote',\n",
       " 'machin',\n",
       " 'lobbyist',\n",
       " 'block',\n",
       " 'elect',\n",
       " 'secur',\n",
       " 'bill',\n",
       " 't',\n",
       " 'rt',\n",
       " 'til',\n",
       " 'after',\n",
       " 'photograph',\n",
       " 'took',\n",
       " 'horrif',\n",
       " 'image',\n",
       " 'wrap',\n",
       " 'phan',\n",
       " 'thi',\n",
       " 'kim',\n",
       " 'phuc',\n",
       " 'blanket',\n",
       " 'drove',\n",
       " 'h',\n",
       " 'rt',\n",
       " 'except',\n",
       " 'everyon',\n",
       " 'live',\n",
       " 'tunisian',\n",
       " 'state',\n",
       " 'rule',\n",
       " 'get',\n",
       " 'vote',\n",
       " 'unlik',\n",
       " 'million',\n",
       " 'palestinian',\n",
       " 'rule',\n",
       " 'israe',\n",
       " 'rt',\n",
       " 'yesterday',\n",
       " 'block',\n",
       " 'civil',\n",
       " 'defens',\n",
       " 'the',\n",
       " 'white',\n",
       " 'helmet',\n",
       " 'page',\n",
       " 'publish',\n",
       " 'content',\n",
       " 'the',\n",
       " 'page',\n",
       " 'h',\n",
       " 'rt',\n",
       " 'black',\n",
       " 'obama',\n",
       " 'era',\n",
       " 'staffer',\n",
       " 'co',\n",
       " 'sign',\n",
       " 'op',\n",
       " 'ed',\n",
       " 'we',\n",
       " 'stand',\n",
       " 'congresswomen',\n",
       " 'ilhan',\n",
       " 'omar',\n",
       " 'alexandria',\n",
       " 'ocasio',\n",
       " 'cortez',\n",
       " 'ayanna',\n",
       " 'rt',\n",
       " 'the',\n",
       " 'detent',\n",
       " 'came',\n",
       " 'around',\n",
       " 'protest',\n",
       " 'demand',\n",
       " 'opposit',\n",
       " 'member',\n",
       " 'allow',\n",
       " 'run',\n",
       " 'local',\n",
       " 'election',\n",
       " 'xa',\n",
       " 'nhttps',\n",
       " 't',\n",
       " 'rt',\n",
       " 'the',\n",
       " 'mosquito',\n",
       " 'are',\n",
       " 'come',\n",
       " 'rt',\n",
       " 'romania',\n",
       " 'polic',\n",
       " 'chief',\n",
       " 'fire',\n",
       " 'after',\n",
       " 'offic',\n",
       " 'took',\n",
       " 'hour',\n",
       " 'respond',\n",
       " 'kidnap',\n",
       " 'girl',\n",
       " 'call',\n",
       " 'rt',\n",
       " 'mathematician',\n",
       " 'like',\n",
       " 'painter',\n",
       " 'poet',\n",
       " 'maker',\n",
       " 'patterns',\n",
       " 'rt',\n",
       " 'sever',\n",
       " 'studi',\n",
       " 'shown',\n",
       " 'civil',\n",
       " 'conflict',\n",
       " 'instabl',\n",
       " 'countri',\n",
       " 'close',\n",
       " 'link',\n",
       " 'climate',\n",
       " 'and',\n",
       " 'that',\n",
       " 'something',\n",
       " 'rt',\n",
       " 'could',\n",
       " 'diagram',\n",
       " 'sentenc',\n",
       " 'everi',\n",
       " 'english',\n",
       " 'cours',\n",
       " 'rest',\n",
       " 'histori',\n",
       " 'still',\n",
       " 'miss',\n",
       " 'point',\n",
       " 'racism',\n",
       " 'often',\n",
       " 'm',\n",
       " 'rt',\n",
       " 'how',\n",
       " 'one',\n",
       " 'small',\n",
       " 'news',\n",
       " 'organization',\n",
       " 'investig',\n",
       " 'report',\n",
       " 'combin',\n",
       " 'power',\n",
       " 'public',\n",
       " 'respons',\n",
       " 'took',\n",
       " 'puerto',\n",
       " 'rico',\n",
       " 'rt',\n",
       " 'the',\n",
       " 'trump',\n",
       " 'crime',\n",
       " 'regim',\n",
       " 'start',\n",
       " 'pay',\n",
       " 'b',\n",
       " 'august',\n",
       " 'farmer',\n",
       " 'hurt',\n",
       " 'trade',\n",
       " 'war',\n",
       " 'tariffs',\n",
       " 'n',\n",
       " 'nthis',\n",
       " 'rt',\n",
       " 'end',\n",
       " 'today',\n",
       " 'this',\n",
       " 'good',\n",
       " 'night',\n",
       " 'nhttps',\n",
       " 'rt',\n",
       " 'the',\n",
       " 'investigatori',\n",
       " 'superpow',\n",
       " 'hous',\n",
       " 'trigger',\n",
       " 'fact',\n",
       " 'impeach',\n",
       " 'inquiri',\n",
       " 'underway',\n",
       " 'the',\n",
       " 'rt',\n",
       " 'brilliant',\n",
       " 'funny',\n",
       " 'wrong',\n",
       " 'compar',\n",
       " 'mantra',\n",
       " 'ferdinand',\n",
       " 's',\n",
       " 'fiat',\n",
       " 'iustitia',\n",
       " 'pereat',\n",
       " 'mundus',\n",
       " 'le',\n",
       " 'rt',\n",
       " 'it',\n",
       " 'worthless',\n",
       " 'trip',\n",
       " 'that',\n",
       " 'way',\n",
       " 'feel',\n",
       " 'said',\n",
       " 'one',\n",
       " 'coal',\n",
       " 'miner',\n",
       " 'travel',\n",
       " 'washington',\n",
       " 'd',\n",
       " 'c',\n",
       " 'me',\n",
       " 'rt',\n",
       " 'cool',\n",
       " 'now',\n",
       " 'oklahoma',\n",
       " 'trailer',\n",
       " 'parks',\n",
       " 'n',\n",
       " 'nhttps',\n",
       " 'rt',\n",
       " 'whi',\n",
       " 'get',\n",
       " 'vote',\n",
       " 'machin',\n",
       " 'trademark',\n",
       " 'china',\n",
       " 'meet',\n",
       " 'brian',\n",
       " 'kemp',\n",
       " 'day',\n",
       " 'georgia',\n",
       " 'rt',\n",
       " 'pay',\n",
       " 'attent',\n",
       " 'the',\n",
       " 'facts',\n",
       " 'n',\n",
       " 'nhel',\n",
       " 'all',\n",
       " 'american',\n",
       " 'ha',\n",
       " 'thi',\n",
       " 'guy',\n",
       " 'well',\n",
       " 'the',\n",
       " 'unit',\n",
       " 'kindom',\n",
       " 'rt',\n",
       " 'look',\n",
       " 'what',\n",
       " 'happen',\n",
       " 'moscow',\n",
       " 'today',\n",
       " 'read',\n",
       " 'tweet',\n",
       " 'rt',\n",
       " 'who',\n",
       " 'we',\n",
       " 'trump',\n",
       " 'even',\n",
       " 'tryin',\n",
       " 'hide',\n",
       " 'william',\n",
       " 'barr',\n",
       " 'person',\n",
       " 'attorney',\n",
       " 'trump',\n",
       " 'see',\n",
       " 'doj',\n",
       " 'rt',\n",
       " 'motherfudger',\n",
       " 'rt',\n",
       " 'rt',\n",
       " 'she',\n",
       " 'liter',\n",
       " 'defin',\n",
       " 'eurotrash',\n",
       " 'n',\n",
       " 'rt',\n",
       " 'mani',\n",
       " 'peopl',\n",
       " 'know',\n",
       " 'work',\n",
       " 'restaur',\n",
       " 'got',\n",
       " 'elected',\n",
       " 'don',\n",
       " 't',\n",
       " 'know',\n",
       " 'why',\n",
       " 'rep',\n",
       " 'open',\n",
       " 'a',\n",
       " 'rt',\n",
       " 'don',\n",
       " 't',\n",
       " 'confus',\n",
       " 'messag',\n",
       " 'messenger',\n",
       " 'say',\n",
       " 'work',\n",
       " 'robert',\n",
       " 'mueller',\n",
       " 'what',\n",
       " 'said',\n",
       " 'drama',\n",
       " 'rt',\n",
       " 'prosecutor',\n",
       " 'year',\n",
       " 'feel',\n",
       " 'compel',\n",
       " 'say',\n",
       " 'realli',\n",
       " 'foolish',\n",
       " 'tweet',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'author',\n",
       " 'pac',\n",
       " 'rt',\n",
       " 'add',\n",
       " 'perjuri',\n",
       " 'list',\n",
       " 'rt',\n",
       " 'holi',\n",
       " 'shit',\n",
       " 'rt',\n",
       " 'thi',\n",
       " 'barbar',\n",
       " 'criminal',\n",
       " 'n',\n",
       " 'ndoesn',\n",
       " 't',\n",
       " 'need',\n",
       " 'disgusting',\n",
       " 'crimin',\n",
       " 'campaign',\n",
       " 'reelection',\n",
       " 'non',\n",
       " 'taayers',\n",
       " 'money',\n",
       " 'n',\n",
       " 'nh',\n",
       " 'need',\n",
       " 'hi',\n",
       " 'rt',\n",
       " 'ain',\n",
       " 't',\n",
       " 'atroci',\n",
       " 'truth',\n",
       " 'rt',\n",
       " 'putin',\n",
       " 'trump',\n",
       " 'kim',\n",
       " 'strip',\n",
       " 'power',\n",
       " 'rt',\n",
       " 'trump',\n",
       " 'say',\n",
       " 'wall',\n",
       " 'eens',\n",
       " 'reimburs',\n",
       " 'other',\n",
       " 'countries',\n",
       " 'plural',\n",
       " 'perhap',\n",
       " 'north',\n",
       " 'south',\n",
       " 'mexico',\n",
       " 'https',\n",
       " 'rt',\n",
       " 'rt',\n",
       " 'scientist',\n",
       " 'say',\n",
       " 'trump',\n",
       " 'administr',\n",
       " 'censor',\n",
       " 'their',\n",
       " 'climat',\n",
       " 'research',\n",
       " 'morgan',\n",
       " 'baskin',\n",
       " 'rt',\n",
       " 'amen',\n",
       " 'mitch',\n",
       " 'mcconnel',\n",
       " 'russian',\n",
       " 'asset',\n",
       " 'rt',\n",
       " 'ross',\n",
       " 'email',\n",
       " 'broidi',\n",
       " 'alert',\n",
       " 'ope',\n",
       " 'planned',\n",
       " 'want',\n",
       " 'let',\n",
       " 'know',\n",
       " 'done',\n",
       " 'piec',\n",
       " 'iran',\n",
       " 'qata',\n",
       " 'rt',\n",
       " 'you',\n",
       " 'r',\n",
       " 'say',\n",
       " 'denni',\n",
       " 'ross',\n",
       " 'paid',\n",
       " 'shill',\n",
       " 'israel',\n",
       " 'arab',\n",
       " 'allies',\n",
       " 'wa',\n",
       " 'naiv',\n",
       " 'believ',\n",
       " 'pro',\n",
       " 'bono',\n",
       " 'hack',\n",
       " 'rt',\n",
       " 'can',\n",
       " 'someon',\n",
       " 'remind',\n",
       " 'north',\n",
       " 'korea',\n",
       " 'got',\n",
       " 'nuclear',\n",
       " 'weapons',\n",
       " 'n',\n",
       " 'noh',\n",
       " 'right',\n",
       " 'pull',\n",
       " 'danger',\n",
       " 'flawed',\n",
       " 'nuclear',\n",
       " 'rt',\n",
       " 'video',\n",
       " 'footag',\n",
       " 'nationalist',\n",
       " 'prime',\n",
       " 'minist',\n",
       " 'muhammad',\n",
       " 'musaddiq',\n",
       " 'led',\n",
       " 'movement',\n",
       " 'nationalis',\n",
       " 'iranian',\n",
       " 'oil',\n",
       " 'rt',\n",
       " 'tonight',\n",
       " 'trump',\n",
       " 'administration',\n",
       " 'remain',\n",
       " 'mexico',\n",
       " 'polici',\n",
       " 'look',\n",
       " 'like',\n",
       " 'danger',\n",
       " 'citi',\n",
       " 'juare',\n",
       " 'rt',\n",
       " 'summer',\n",
       " 'heat',\n",
       " 'wave',\n",
       " 'wildfir',\n",
       " 'link',\n",
       " 'climat',\n",
       " 'chang',\n",
       " 'thi',\n",
       " 'climat',\n",
       " 'chang',\n",
       " 'look',\n",
       " 'like',\n",
       " 'rt',\n",
       " 'pick',\n",
       " 'dem',\n",
       " 'presidenti',\n",
       " 'candid',\n",
       " 'least',\n",
       " 'october',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'say',\n",
       " 'support',\n",
       " 'thing',\n",
       " 'ca',\n",
       " 'rt',\n",
       " 'publish',\n",
       " 'the',\n",
       " 'homosexual',\n",
       " 'newli',\n",
       " 'visible',\n",
       " 'newli',\n",
       " 'understood',\n",
       " 'while',\n",
       " 'controversi',\n",
       " 'piec',\n",
       " 'discuss',\n",
       " 'pu',\n",
       " 'rt',\n",
       " 'studi',\n",
       " 'harlem',\n",
       " 'mid',\n",
       " 's',\n",
       " 'knight',\n",
       " 'said',\n",
       " 'mentor',\n",
       " 'bi',\n",
       " 'look',\n",
       " 'her',\n",
       " 'rt',\n",
       " 'open',\n",
       " 'art',\n",
       " 'galleri',\n",
       " 'dedic',\n",
       " 'sale',\n",
       " 'amp',\n",
       " 'exhibit',\n",
       " 'black',\n",
       " 'art',\n",
       " 'rt',\n",
       " 'gt',\n",
       " 'n',\n",
       " 'nhow',\n",
       " 'six',\n",
       " 'femal',\n",
       " 'photojournalist',\n",
       " 'saw',\n",
       " 'world',\n",
       " 'accord',\n",
       " 'photograph',\n",
       " 'life',\n",
       " 'magazin',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'birthday',\n",
       " 'i',\n",
       " 'll',\n",
       " 'cri',\n",
       " 'want',\n",
       " 'the',\n",
       " 'smirk',\n",
       " 'chimp',\n",
       " 'trump',\n",
       " 'final',\n",
       " 'do',\n",
       " 'harm',\n",
       " 'obama',\n",
       " 'revit',\n",
       " 'economi',\n",
       " 'the',\n",
       " 'smirk',\n",
       " 'chimp',\n",
       " 'the',\n",
       " 'myth',\n",
       " 'trump',\n",
       " 'economy',\n",
       " 'go',\n",
       " 'flame',\n",
       " 'the',\n",
       " 'smirk',\n",
       " 'chimp',\n",
       " 'rt',\n",
       " 'terrifying',\n",
       " 'rt',\n",
       " 'sad',\n",
       " 'necessari',\n",
       " 'rt',\n",
       " 'call',\n",
       " 'rt',\n",
       " 'rt',\n",
       " 'hope',\n",
       " 'judiciari',\n",
       " 'committe',\n",
       " 'big',\n",
       " 'enough',\n",
       " 'trap',\n",
       " 'trump',\n",
       " 'rt',\n",
       " 'ihr',\n",
       " 'papier',\n",
       " 'bitte',\n",
       " 'rt',\n",
       " 'traitor',\n",
       " 'mitch',\n",
       " 'donors',\n",
       " 'rt',\n",
       " 'state',\n",
       " 'sen',\n",
       " 'lauren',\n",
       " 'book',\n",
       " 'say',\n",
       " 'warn',\n",
       " 'back',\n",
       " 'jeffrey',\n",
       " 'epstein',\n",
       " 'case',\n",
       " 'littl',\n",
       " 'girl',\n",
       " 'don',\n",
       " 't',\n",
       " 'know',\n",
       " 'you',\n",
       " 'r',\n",
       " 'gett',\n",
       " 'rt',\n",
       " 'mcconnel',\n",
       " 'fire',\n",
       " 'buri',\n",
       " 'elect',\n",
       " 'bill',\n",
       " 'legisl',\n",
       " 'graveyard',\n",
       " 'thehil',\n",
       " 'rt',\n",
       " 'everyon',\n",
       " 'knows',\n",
       " 'stand',\n",
       " 'bulli',\n",
       " 'often',\n",
       " 'tri',\n",
       " 'play',\n",
       " 'victim',\n",
       " 'frame',\n",
       " 'bully',\n",
       " 'n',\n",
       " 'nso',\n",
       " 'course',\n",
       " 'rt',\n",
       " 'russian',\n",
       " 'sandals',\n",
       " 'rt',\n",
       " 'anyon',\n",
       " 'fish',\n",
       " 'rt',\n",
       " 'one',\n",
       " 'ribbon',\n",
       " 'golfing',\n",
       " 'one',\n",
       " 'bowling',\n",
       " 'one',\n",
       " 'best',\n",
       " 'words',\n",
       " 'one',\n",
       " 'kite',\n",
       " 'flying',\n",
       " 'anoth',\n",
       " 'color',\n",
       " 'numb',\n",
       " 'rt',\n",
       " 'bewar',\n",
       " 'fals',\n",
       " 'prophets',\n",
       " 'come',\n",
       " 'sheep',\n",
       " 'clothing',\n",
       " 'inwardli',\n",
       " 'raven',\n",
       " 'wolves',\n",
       " 'rt',\n",
       " 'you',\n",
       " 'want',\n",
       " 'believ',\n",
       " 'conserv',\n",
       " 'rel',\n",
       " 'better',\n",
       " 'trump',\n",
       " 'they',\n",
       " 'aren',\n",
       " 't',\n",
       " 'they',\n",
       " 'might',\n",
       " 'possibl',\n",
       " 'better',\n",
       " 'hair',\n",
       " 'rt',\n",
       " 'let',\n",
       " 'say',\n",
       " 'again',\n",
       " 'support',\n",
       " 'racist',\n",
       " 'part',\n",
       " 'racism',\n",
       " 'don',\n",
       " 't',\n",
       " 'care',\n",
       " 'you',\n",
       " 'r',\n",
       " 'grandma',\n",
       " 'wh',\n",
       " 'rt',\n",
       " 'elijah',\n",
       " 'cum',\n",
       " 'grew',\n",
       " 'face',\n",
       " 'racist',\n",
       " 'bulli',\n",
       " 'like',\n",
       " 'trump',\n",
       " 'learn',\n",
       " 'confront',\n",
       " 'qualiti',\n",
       " 'unknown',\n",
       " 'trump',\n",
       " 'rt',\n",
       " 'rememb',\n",
       " 'mark',\n",
       " 'meadow',\n",
       " 'got',\n",
       " 'indign',\n",
       " 'charg',\n",
       " 'racial',\n",
       " 'insensitivity',\n",
       " 'point',\n",
       " 'friendship',\n",
       " 'elij',\n",
       " 'rt',\n",
       " 'thi',\n",
       " 'one',\n",
       " 'got',\n",
       " 'block',\n",
       " 'rt',\n",
       " 'who',\n",
       " 'own',\n",
       " 'maryland',\n",
       " 'apart',\n",
       " 'infest',\n",
       " 'rat',\n",
       " 'amp',\n",
       " 'black',\n",
       " 'mold',\n",
       " 'who',\n",
       " 'evict',\n",
       " 'tenant',\n",
       " 'dare',\n",
       " 'complain',\n",
       " 'amp',\n",
       " 'takes',\n",
       " 'rt',\n",
       " 'disappoint',\n",
       " 'mueller',\n",
       " 'clear',\n",
       " 'whole',\n",
       " 'nightmare',\n",
       " 'don',\n",
       " 't',\n",
       " 'need',\n",
       " 'wow',\n",
       " 'de',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc_bow = dictionary.doc2bow(test_rdf.iloc[random_test_doc_index,3])\n",
    "test_rdf.iloc[random_test_doc_index,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions from the random document in the test set:\n",
    "test_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=test_doc_bow)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visuliase distribution from the test document using simialr approach:\n",
    "fig, ax = plt.subplots(figsize=(9,3));\n",
    "patches = ax.bar(np.arange(len(test_doc_distribution)), test_doc_distribution)\n",
    "ax.set_xlabel('Topic ID', fontsize=13)\n",
    "ax.set_ylabel('Topic Contribution', fontsize=13)\n",
    "#ax.set_title('Topic Distibution for Test Document ' + str(random_doc_index), fontsize=17)\n",
    "ax.set_title('Main tokens: '+' '.join([t[0] for t in lda.show_topic(topicid=random_doc_index, topn=13) if len(t[0])>1]))\n",
    "\n",
    "ax.set_xticks(np.linspace(3,30,10))\n",
    "fig.tight_layout()\n",
    "#plt.savefig('results/topic-distribution-for-random-test-document.png',dp1=321)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [('rt', 0.071895674), ('n', 0.01687156), ('the', 0.01575746), ('t', 0.008352737), ('trump', 0.006973596), ('amp', 0.0069507626), ('it', 0.006302818)] \n",
      "\n",
      "10 [('i', 0.029627936), ('you', 0.029362097), ('t', 0.019512776), ('it', 0.01923757), ('m', 0.018384919), ('one', 0.013786593), ('rt', 0.01280012)] \n",
      "\n",
      "7 [('n', 0.87550956), ('you', 0.019605702), ('racist', 0.017237892), ('na', 0.011738921), ('support', 0.007219665), ('nwhat', 0.006757225), ('that', 0.006657888)] \n",
      "\n",
      "12 [('the', 0.08124576), ('syrian', 0.05086078), ('now', 0.039046958), ('film', 0.035821207), ('listen', 0.03156211), ('playing', 0.025355732), ('space', 0.020406995)] \n",
      "\n",
      "0 [('rt', 0.108258896), ('que', 0.08544535), ('n', 0.08023813), ('a', 0.04262468), ('d', 0.037150234), ('m', 0.03693572), ('est', 0.03190444)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top terms in the distribution:\n",
    "for i in test_doc_distribution.argsort()[-5:][::-1]:\n",
    "    print(i, lda.show_topic(topicid=i, topn=7), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMPARING DOCUMENTS**\n",
    "Because each of the document (or the set of tweets from each node) can be represented as a distribution of contributing topics, the distributions of document is compared for assessing the simiarity between documents in the corpus. Each documnet is compared with other documents in the corpuse and the most similar documents to the document (known as the *anchor document*) are returned. The *Jensen-Shannon* is a suitable statistical metric to measure the distance/simialriy between the documents using their distributions. The divergence in the distirbutions of the document is used to assess simialrity. Proximity is crucial in this case - the closer or similar the document, the less the divergence/distance and vice versa.\n",
    "\n",
    "**Jensen-Shannon Metric** is a symmetrical version of the asymmetric *Kullback-Leibler Divergence*. The symmerical property is relevant since the task of comparing two documents should be the same irrespective of the order - A-->B or B-->A should be the same; the *Jensen-Shannon* meets this requirement. Given two discrete distributions $A$ and $B$, the *Jensen-Shannon Divergence (JSD)* is given by/define by $$JS_{divergence}(A||B) = \\frac{1}{2}D(A||M) + \\frac{1}{2}D(B||M)$$\n",
    "where $$M = \\frac{1}{2}(A+B)$$ denotes the mean of the distributons and the *Kullback-Leibler Divergence D* given by\n",
    "$$D(A||B) = \\sum_i A(i)\\log\\frac{A(i)}{B(i)}$$\n",
    "and substituted in **Eq. 1**\n",
    "\n",
    "$$JS_{divergence}(A||B) = \\frac{1}{2}\\sum_i\\left[A(i)\\log\\left(\\frac{A(i)}{\\frac{1}{2}(A(i)+B(i))}\\right) + B(i)\\log\\left(\\frac{B(i)}{\\frac{1}{2}(A(i)+B(i))}\\right)\\right]$$\n",
    "\n",
    "The distance measure of *JSD* is obtained by squaring its divergence relation given by $$JS_{dist} = \\sqrt{ \\left(JS_{divergence}(A||B \\right)}$$\n",
    "\n",
    "Both $(JS_{divergence}(A||B)$ and $JS_{dist}$ have been computed using the *Scipy implementation* (see https://www.scipy.org/) which is based on the entropy (which calculates the K-L divergence $(JS_{divergence}(A||B)$.)\n",
    "\n",
    "**Matrix of Values** In order ro enable faster/efficient computation noting the huge size of the data), all the topics distributions learned by the topic model are represented in a dense matrix $M_{lda}^{m\\times k}$. The matrix $M_{lda}^{m\\times k}$ of size $M\\times K$ is a dense matrix consisting of $M$ number of documents (set of tweets from nodes) and $K$ their corresponding number of *topics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 25)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dense matrix creation:\n",
    "doc_topic_dist = np.array([[tup[1] for tup in t1] for t1 in lda[corpus]])\n",
    "doc_topic_dist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the matrix is 129, 25 i.e. 129 documents and 25 topics. In other words, all the discussions topics in the 129 nodes is within the set of 25 topics and each node may discus from 0 to 25 topics, hence similar nodes will discuss simialr topics.\n",
    "\n",
    "*Helper functions:*\n",
    "- A function to compute the *JS distance*. The function accepts a *query document* and compare with all other *documents* in the matrix. Because the matrix is an $M \\times K$ matrix, the output from issuing the query will return an array of size $M$ i.e. size of the corpus or number of documents in the corpus each with a degree of similarity with the query document. Only the top similar documenst are used in this study.\n",
    "- A function to return the *indices of top similar documents* in the matrix to the anchor document. The indices can be used to compare/see the returned document by checking the train_rdf for the specific documents. Using the distribution of the random document and all other distributions in the dense matrix using the indices of the most similar docs to the anchor doc are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute JS-distance \n",
    "def js_distance(anchor, matrix):\n",
    "    a = anchor[None,:].T # a transposed the anchor or query document\n",
    "    b = matrix.T #transpose of the matix to align the inner dimensions of the documents\n",
    "    m = 0.5*(a + b) # mean of the two i.e. m from the relation given earlier\n",
    "    return np.sqrt(0.5*(entropy(a,m) + entropy(b,m))) # entropy is the K-L divergence    \n",
    "\n",
    "# function to return the indices of top similar documents in the matrix to the anchor document\n",
    "def similar_docs(anchor,matrix,k=7):\n",
    "    jsims = js_distance(anchor,matrix) #compute simialrity by invoking the js_distance function:\n",
    "    return jsims.argsort()[:k] # return the top k documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the comparison space is huge, ~100k for each comparison, the process is paralellise for efficiency ..   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # run and check the output ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.17 ms, sys: 0 ns, total: 2.17 ms\n",
      "Wall time: 4.75 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ijdutse/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in sqrt\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# apply the functions:\n",
    "%time sim_docs_idx = similar_docs(test_doc_distribution, doc_topic_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # get/check actual documents from the train_rdf: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_docs = train_rdf[train_rdf.index.isin(sim_docs_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>CleanTweets</th>\n",
       "      <th>Shingles</th>\n",
       "      <th>TweetsLen</th>\n",
       "      <th>CleanTweetsLen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>783760671993462789</td>\n",
       "      <td>['the nut cadet blue narcissuses are gray mcch...</td>\n",
       "      <td>the nut cadet blue narcissus gray mcchicken ...</td>\n",
       "      <td>[the, nut, cadet, blue, narcissus, gray, mcchi...</td>\n",
       "      <td>980</td>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2169143610</td>\n",
       "      <td>['@ThomJoanna @yellowtail @sainsburys ... Grim...</td>\n",
       "      <td>grim grape    intrigu far scottish tori wo...</td>\n",
       "      <td>[grim, grape, intrigu, far, scottish, tori, wo...</td>\n",
       "      <td>1799</td>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1363099992</td>\n",
       "      <td>[\"RT @mattcope16: There's gold in this blog: T...</td>\n",
       "      <td>rt there  gold blog  the earli transact stuf...</td>\n",
       "      <td>[rt, there, gold, blog, the, earli, transact, ...</td>\n",
       "      <td>1614</td>\n",
       "      <td>948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>839718993724321792</td>\n",
       "      <td>['RT @WalshFreedom: Either Donald Trump hates ...</td>\n",
       "      <td>rt either donald trump hate black brown peop...</td>\n",
       "      <td>[rt, either, donald, trump, hate, black, brown...</td>\n",
       "      <td>1641</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>119439864</td>\n",
       "      <td>['@jessphillips Very happy to help change your...</td>\n",
       "      <td>veri happi help chang name jess phillip pm    ...</td>\n",
       "      <td>[veri, happi, help, chang, name, jess, phillip...</td>\n",
       "      <td>1245</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>864999726814347264</td>\n",
       "      <td>['RT @CampbellACoope1: This @POTUS gets it! RU...</td>\n",
       "      <td>rt thi get it  run pay brave men  amp  women...</td>\n",
       "      <td>[rt, thi, get, it, run, pay, brave, men, amp, ...</td>\n",
       "      <td>1699</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                UserID                                             Tweets  \\\n",
       "5   783760671993462789  ['the nut cadet blue narcissuses are gray mcch...   \n",
       "53          2169143610  ['@ThomJoanna @yellowtail @sainsburys ... Grim...   \n",
       "58          1363099992  [\"RT @mattcope16: There's gold in this blog: T...   \n",
       "62  839718993724321792  ['RT @WalshFreedom: Either Donald Trump hates ...   \n",
       "78           119439864  ['@jessphillips Very happy to help change your...   \n",
       "80  864999726814347264  ['RT @CampbellACoope1: This @POTUS gets it! RU...   \n",
       "\n",
       "                                          CleanTweets  \\\n",
       "5     the nut cadet blue narcissus gray mcchicken ...   \n",
       "53      grim grape    intrigu far scottish tori wo...   \n",
       "58    rt there  gold blog  the earli transact stuf...   \n",
       "62    rt either donald trump hate black brown peop...   \n",
       "78  veri happi help chang name jess phillip pm    ...   \n",
       "80    rt thi get it  run pay brave men  amp  women...   \n",
       "\n",
       "                                             Shingles  TweetsLen  \\\n",
       "5   [the, nut, cadet, blue, narcissus, gray, mcchi...        980   \n",
       "53  [grim, grape, intrigu, far, scottish, tori, wo...       1799   \n",
       "58  [rt, there, gold, blog, the, earli, transact, ...       1614   \n",
       "62  [rt, either, donald, trump, hate, black, brown...       1641   \n",
       "78  [veri, happi, help, chang, name, jess, phillip...       1245   \n",
       "80  [rt, thi, get, it, run, pay, brave, men, amp, ...       1699   \n",
       "\n",
       "    CleanTweetsLen  \n",
       "5              865  \n",
       "53            1017  \n",
       "58             948  \n",
       "62            1007  \n",
       "78             716  \n",
       "80            1033  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar documents or clusters to the anchor:\n",
    "sim_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These five documents are the most similar to the achor document and constitutes the circles or set of textually similar items with the anchor, hence belonging to same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results representation in a matrix form for further analysis:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sociometry & spectral clustering:** Now that nodes with high likelihood of reciprocity have been idenitifed, the next step in te pipeline is to utilise the nodes for data collection. However, there is a need to compute relevant metrics for sociometry and spectral clustering. Emphasis is on centrality measures -- *degree, closeness and betweeness* on various datasets (empirical dataset, actual users with reciprocal ties, and predicted data i.e. based on the likelihood of reciprocity).\n",
    "\n",
    "***Extract more features ...*** could be used for further analysis involving *sociometry* ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code snippets to retrieve relevant features for further analysis involving sociometry ...\n",
    "start = time.time()\n",
    "#texts = defaultdict(list)\n",
    "extracts3 = defaultdict(list)\n",
    "for chunk in np.array_split(np.array(users), 22): # split the users' array into 22 chunks of sizes <100 to avoid limit\n",
    "    chunk = chunk.tolist() # convert to list\n",
    "    buddies = api.lookup_users(user_ids=chunk)#[i for i in users])#use the usernames to get set of m tweets for LDA \n",
    "    for p in buddies:\n",
    "        try:\n",
    "            extracts3['UserID'].append(p.id)\n",
    "            extracts3['ScreenName'].append(p.screen_name)\n",
    "            extracts3['CreatedAt'].append(p.created_at)\n",
    "            extracts3['Followers'].append(p.followers_count)\n",
    "            extracts3['Friends'].append(p.friends_count)\n",
    "            extracts3['Statuses'].append(p.statuses_count)\n",
    "            extracts3['Description'].append(p.description)\n",
    "            extracts3['Location'].append(p.location)\n",
    "            extracts3['Favourite'].append(p.favourites_count)\n",
    "            extracts3['Verification'].append(p.verified)\n",
    "            if hasattr(p, 'status'):\n",
    "                extracts3['Texts'].append(p.status.text)\n",
    "                extracts3['RTCount'].append(p.status.retweet_count)\n",
    "                extracts3['AccountCreated'].append(p.status.created_at)\n",
    "            else:\n",
    "                extracts3['Texts'].append('nan')\n",
    "                extracts3['RTCount'].append('nan')\n",
    "                extracts3['AccountCreated'].append('nan')\n",
    "            ########################\n",
    "            #save file:\n",
    "            d = pd.DataFrame(extracts3) \n",
    "            with open ('data/mct_features_and_text_extracts_from_sr_nodes.csv','a') as csv:\n",
    "                d.to_csv(csv, header=False, mode='a', index_label=False)\n",
    "        except tweepy.TweepError:\n",
    "            time.sleep(5*15)\n",
    "            continue\n",
    "        except StopIteration:\n",
    "            break    \n",
    "stop = time.time()-start\n",
    "print('The process took: ',stop/60, ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1695561"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read extracts:\n",
    "dff = pd.read_csv('data/mct_features_and_text_extracts_from_sr_nodes.csv', names=['UserID', 'ScreenName','CreatedAt',\\\n",
    "                            'Followers','Friends','Statuses','Description','Location','Favourite','Verification',\\\n",
    "                            'Texts','RTCount','AccountCreated'])\n",
    "len(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>CreatedAt</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Friends</th>\n",
       "      <th>Statuses</th>\n",
       "      <th>Description</th>\n",
       "      <th>Location</th>\n",
       "      <th>Favourite</th>\n",
       "      <th>Verification</th>\n",
       "      <th>Texts</th>\n",
       "      <th>RTCount</th>\n",
       "      <th>AccountCreated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1093985024171540480</td>\n",
       "      <td>iamola_tokunbo</td>\n",
       "      <td>2019-02-08 21:28:49</td>\n",
       "      <td>2609</td>\n",
       "      <td>2896</td>\n",
       "      <td>577</td>\n",
       "      <td>Be the good you want to see in others</td>\n",
       "      <td>Lagos, Nigeria</td>\n",
       "      <td>7695</td>\n",
       "      <td>False</td>\n",
       "      <td>5 people followed me and 6 people unfollowed m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-24 21:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1093985024171540480</td>\n",
       "      <td>iamola_tokunbo</td>\n",
       "      <td>2019-02-08 21:28:49</td>\n",
       "      <td>2609</td>\n",
       "      <td>2896</td>\n",
       "      <td>577</td>\n",
       "      <td>Be the good you want to see in others</td>\n",
       "      <td>Lagos, Nigeria</td>\n",
       "      <td>7695</td>\n",
       "      <td>False</td>\n",
       "      <td>5 people followed me and 6 people unfollowed m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-24 21:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822379141773148161</td>\n",
       "      <td>Gatopatriota</td>\n",
       "      <td>2017-01-20 09:44:01</td>\n",
       "      <td>334</td>\n",
       "      <td>445</td>\n",
       "      <td>9286</td>\n",
       "      <td>Si algún día consigo el dinero, haré el mayor ...</td>\n",
       "      <td>Spain</td>\n",
       "      <td>9052</td>\n",
       "      <td>False</td>\n",
       "      <td>Así encabeza su primer correo del curso la res...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-10 20:35:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                UserID      ScreenName            CreatedAt  Followers  \\\n",
       "0  1093985024171540480  iamola_tokunbo  2019-02-08 21:28:49       2609   \n",
       "0  1093985024171540480  iamola_tokunbo  2019-02-08 21:28:49       2609   \n",
       "1   822379141773148161    Gatopatriota  2017-01-20 09:44:01        334   \n",
       "\n",
       "   Friends  Statuses                                        Description  \\\n",
       "0     2896       577              Be the good you want to see in others   \n",
       "0     2896       577              Be the good you want to see in others   \n",
       "1      445      9286  Si algún día consigo el dinero, haré el mayor ...   \n",
       "\n",
       "         Location  Favourite  Verification  \\\n",
       "0  Lagos, Nigeria       7695         False   \n",
       "0  Lagos, Nigeria       7695         False   \n",
       "1           Spain       9052         False   \n",
       "\n",
       "                                               Texts  RTCount  \\\n",
       "0  5 people followed me and 6 people unfollowed m...      0.0   \n",
       "0  5 people followed me and 6 people unfollowed m...      0.0   \n",
       "1  Así encabeza su primer correo del curso la res...      0.0   \n",
       "\n",
       "        AccountCreated  \n",
       "0  2019-09-24 21:03:40  \n",
       "0  2019-09-24 21:03:40  \n",
       "1  2019-09-10 20:35:04  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
